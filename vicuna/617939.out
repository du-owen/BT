Running on host: tikgpu05
In directory: /itet-stor/owendu/net_scratch/Paraphrase/FastChat/paraphrase
Starting on: Thu May 11 15:04:30 CEST 2023
SLURM_JOB_ID: 617939
/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/libcudart.so'), PosixPath('/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.
Either way, this might cause trouble in the future:
If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
  warn(msg)
/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!
  warn(msg)
2023-05-11 15:05:36,231	INFO worker.py:1616 -- Started a local Ray instance. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
[2m[36m(pid=160126)[0m /itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/libcudart.so'), PosixPath('/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.
[2m[36m(pid=160126)[0m Either way, this might cause trouble in the future:
[2m[36m(pid=160126)[0m If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
[2m[36m(pid=160126)[0m   warn(msg)
[2m[36m(pid=160126)[0m /itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!
[2m[36m(pid=160126)[0m   warn(msg)
[2m[36m(get_model_answers pid=160126)[0m Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
[2m[36m(get_model_answers pid=160126)[0m Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:05<00:10,  5.25s/it]
[2m[36m(get_model_answers pid=160126)[0m Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:05,  5.27s/it]
[2m[36m(get_model_answers pid=160126)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.47s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.68s/it]
[2m[36m(get_model_answers pid=160126)[0m   0%|          | 0/9 [00:00<?, ?it/s]
[2m[36m(get_model_answers pid=160126)[0m /itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
[2m[36m(get_model_answers pid=160126)[0m   warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2m[36m(get_model_answers pid=160126)[0m   0%|          | 0/9 [00:02<?, ?it/s]

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113_nocublaslt.so
CUDA SETUP: CUDA runtime path found: /itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 7.0
CUDA SETUP: Detected CUDA version 113
CUDA SETUP: Loading binary /itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113_nocublaslt.so...
[2m[36m(pid=160126)[0m 
[2m[36m(pid=160126)[0m ===================================BUG REPORT===================================
[2m[36m(pid=160126)[0m Welcome to bitsandbytes. For bug reports, please run
[2m[36m(pid=160126)[0m 
[2m[36m(pid=160126)[0m python -m bitsandbytes
[2m[36m(pid=160126)[0m 
[2m[36m(pid=160126)[0m  and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
[2m[36m(pid=160126)[0m ================================================================================
[2m[36m(pid=160126)[0m bin /itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113_nocublaslt.so
[2m[36m(pid=160126)[0m CUDA SETUP: CUDA runtime path found: /itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/libcudart.so
[2m[36m(pid=160126)[0m CUDA SETUP: Highest compute capability among GPUs detected: 7.0
[2m[36m(pid=160126)[0m CUDA SETUP: Detected CUDA version 113
[2m[36m(pid=160126)[0m CUDA SETUP: Loading binary /itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113_nocublaslt.so...
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ /usr/itetnas04/data-scratch-01/owendu/data/Paraphrase/FastChat/paraphrase/.. â”‚
â”‚ /fastchat/eval/get_model_answer.py:92 in <module>                            â”‚
â”‚                                                                              â”‚
â”‚   89 â”‚   args = parser.parse_args()                                          â”‚
â”‚   90 â”‚                                                                       â”‚
â”‚   91 â”‚   ray.init(num_cpus=1)                                                â”‚
â”‚ â± 92 â”‚   run_eval(                                                           â”‚
â”‚   93 â”‚   â”‚   args.model_path,                                                â”‚
â”‚   94 â”‚   â”‚   args.model_id,                                                  â”‚
â”‚   95 â”‚   â”‚   args.question_file,                                             â”‚
â”‚                                                                              â”‚
â”‚ /usr/itetnas04/data-scratch-01/owendu/data/Paraphrase/FastChat/paraphrase/.. â”‚
â”‚ /fastchat/eval/get_model_answer.py:33 in run_eval                            â”‚
â”‚                                                                              â”‚
â”‚   30 â”‚                                                                       â”‚
â”‚   31 â”‚   ans_jsons = []                                                      â”‚
â”‚   32 â”‚   for ans_handle in ans_handles:                                      â”‚
â”‚ â± 33 â”‚   â”‚   ans_jsons.extend(ray.get(ans_handle))                           â”‚
â”‚   34 â”‚                                                                       â”‚
â”‚   35 â”‚   with open(os.path.expanduser(answer_file), "w") as ans_file:        â”‚
â”‚   36 â”‚   â”‚   for line in ans_jsons:                                          â”‚
â”‚                                                                              â”‚
â”‚ /itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-pack â”‚
â”‚ ages/ray/_private/client_mode_hook.py:105 in wrapper                         â”‚
â”‚                                                                              â”‚
â”‚   102 â”‚   â”‚   â”‚   # we only convert init function if RAY_CLIENT_MODE=1       â”‚
â”‚   103 â”‚   â”‚   â”‚   if func.__name__ != "init" or is_client_mode_enabled_by_de â”‚
â”‚   104 â”‚   â”‚   â”‚   â”‚   return getattr(ray, func.__name__)(*args, **kwargs)    â”‚
â”‚ â± 105 â”‚   â”‚   return func(*args, **kwargs)                                   â”‚
â”‚   106 â”‚                                                                      â”‚
â”‚   107 â”‚   return wrapper                                                     â”‚
â”‚   108                                                                        â”‚
â”‚                                                                              â”‚
â”‚ /itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-pack â”‚
â”‚ ages/ray/_private/worker.py:2521 in get                                      â”‚
â”‚                                                                              â”‚
â”‚   2518 â”‚   â”‚   â”‚   â”‚   if isinstance(value, ray.exceptions.ObjectLostError): â”‚
â”‚   2519 â”‚   â”‚   â”‚   â”‚   â”‚   worker.core_worker.dump_object_store_memory_usage â”‚
â”‚   2520 â”‚   â”‚   â”‚   â”‚   if isinstance(value, RayTaskError):                   â”‚
â”‚ â± 2521 â”‚   â”‚   â”‚   â”‚   â”‚   raise value.as_instanceof_cause()                 â”‚
â”‚   2522 â”‚   â”‚   â”‚   â”‚   else:                                                 â”‚
â”‚   2523 â”‚   â”‚   â”‚   â”‚   â”‚   raise value                                       â”‚
â”‚   2524                                                                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
RayTaskError(OutOfMemoryError): [36mray::get_model_answers()[39m (pid=160126, 
ip=129.132.190.223)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "../fastchat/eval/get_model_answer.py", line 60, in get_model_answers
    output_ids = model.generate(
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/transformers/generation/utils.py", line 1485, in generate
    return self.sample(
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/transformers/generation/utils.py", line 2524, in sample
    outputs = self(
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/transformers/models/llama/modeling_llama.py", line 687, in forward
    outputs = self.model(
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/transformers/models/llama/modeling_llama.py", line 577, in forward
    layer_outputs = decoder_layer(
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/transformers/models/llama/modeling_llama.py", line 305, in forward
    hidden_states = self.mlp(hidden_states)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/transformers/models/llama/modeling_llama.py", line 157, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/bitsandbytes/nn/modules.py", line 320, in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/bitsandbytes/autograd/_functions.py", line 500, in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File 
"/itet-stor/owendu/net_scratch/miniconda3/envs/vicuna/lib/python3.8/site-package
s/bitsandbytes/autograd/_functions.py", line 382, in forward
    state.subB = (outliers * state.SCB.view(-1, 1) / 
127.0).t().contiguous().to(A.dtype)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB 
(GPU 0; 31.75 GiB total capacity; 29.27 GiB already allocated; 141.50 MiB free; 
30.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated 
memory try setting max_split_size_mb to avoid fragmentation.  See documentation 
for Memory Management and PYTORCH_CUDA_ALLOC_CONF
